{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d42ad2-761c-46bb-a05d-e7e0aceb020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/qwang/.conda/envs/control/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gridsan/qwang/.conda/envs/control/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/gridsan/qwang/.conda/envs/control/lib/python3.8/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/gridsan/qwang/urban-control/\")\n",
    "\n",
    "from src.train_controlnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bc299b-d216-42a6-a34c-3478b377e3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--tracker_project_name'], dest='tracker_project_name', nargs=None, const=None, default='train_controlnet', type=<class 'str'>, choices=None, help='The `project_name` argument passed to Accelerator.init_trackers for more information see https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Simple example of a ControlNet training script.\")\n",
    "parser.add_argument(\n",
    "    \"--pretrained_model_name_or_path\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    required=True,\n",
    "    help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--controlnet_model_name_or_path\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Path to pretrained controlnet model or model identifier from huggingface.co/models.\"\n",
    "    \" If not specified controlnet weights are initialized from unet.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--revision\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    required=False,\n",
    "    help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--variant\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    type=str,\n",
    "    default=\"controlnet-model\",\n",
    "    help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"The directory where the downloaded models and datasets will be stored.\",\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n",
    "parser.add_argument(\n",
    "    \"--resolution\",\n",
    "    type=int,\n",
    "    default=512,\n",
    "    help=(\n",
    "        \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n",
    "        \" resolution\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_batch_size\", type=int, default=4, help=\"Batch size (per device) for the training dataloader.\"\n",
    ")\n",
    "parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n",
    "parser.add_argument(\n",
    "    \"--max_train_steps\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpointing_steps\",\n",
    "    type=int,\n",
    "    default=500,\n",
    "    help=(\n",
    "        \"Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training via `--resume_from_checkpoint`. \"\n",
    "        \"In the case that the checkpoint is better than the final trained model, the checkpoint can also be used for inference.\"\n",
    "        \"Using a checkpoint for inference requires separate loading of the original pipeline and the individual checkpointed model components.\"\n",
    "        \"See https://huggingface.co/docs/diffusers/main/en/training/dreambooth#performing-inference-using-a-saved-checkpoint for step by step\"\n",
    "        \"instructions.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoints_total_limit\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=(\"Max number of checkpoints to store.\"),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_checkpoint\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=(\n",
    "        \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n",
    "        ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_checkpointing\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\",\n",
    "    type=float,\n",
    "    default=5e-6,\n",
    "    help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--scale_lr\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler\",\n",
    "    type=str,\n",
    "    default=\"constant\",\n",
    "    help=(\n",
    "        'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n",
    "        ' \"constant\", \"constant_with_warmup\"]'\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_num_cycles\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\",\n",
    ")\n",
    "parser.add_argument(\"--lr_power\", type=float, default=1.0, help=\"Power factor of the polynomial scheduler.\")\n",
    "parser.add_argument(\n",
    "    \"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataloader_num_workers\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=(\n",
    "        \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n",
    "parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n",
    "parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n",
    "parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n",
    "parser.add_argument(\n",
    "    \"--hub_model_id\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--logging_dir\",\n",
    "    type=str,\n",
    "    default=\"logs\",\n",
    "    help=(\n",
    "        \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "        \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--allow_tf32\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n",
    "        \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--report_to\",\n",
    "    type=str,\n",
    "    default=\"tensorboard\",\n",
    "    help=(\n",
    "        'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "        ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixed_precision\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "    help=(\n",
    "        \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "        \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "        \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--set_grads_to_none\",\n",
    "    action=\"store_true\",\n",
    "    help=(\n",
    "        \"Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain\"\n",
    "        \" behaviors, so disable this argument if it causes any problems. More info:\"\n",
    "        \" https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=(\n",
    "        \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n",
    "        \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n",
    "        \" or to a folder containing files that 🤗 Datasets can understand.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_config_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"The config of the Dataset, leave as None if there's only one config.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_data_dir\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=(\n",
    "        \"A folder containing the training data. Folder contents must follow the structure described in\"\n",
    "        \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n",
    "        \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--image_column\", type=str, default=\"image\", help=\"The column of the dataset containing the target image.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--conditioning_image_column\",\n",
    "    type=str,\n",
    "    default=\"conditioning_image\",\n",
    "    help=\"The column of the dataset containing the controlnet conditioning image.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--caption_column\",\n",
    "    type=str,\n",
    "    default=\"text\",\n",
    "    help=\"The column of the dataset containing a caption or a list of captions.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_train_samples\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=(\n",
    "        \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "        \"value if set.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--proportion_empty_prompts\",\n",
    "    type=float,\n",
    "    default=0,\n",
    "    help=\"Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--validation_prompt\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    nargs=\"+\",\n",
    "    help=(\n",
    "        \"A set of prompts evaluated every `--validation_steps` and logged to `--report_to`.\"\n",
    "        \" Provide either a matching number of `--validation_image`s, a single `--validation_image`\"\n",
    "        \" to be used with all prompts, or a single prompt that will be used with all `--validation_image`s.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--validation_image\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    nargs=\"+\",\n",
    "    help=(\n",
    "        \"A set of paths to the controlnet conditioning image be evaluated every `--validation_steps`\"\n",
    "        \" and logged to `--report_to`. Provide either a matching number of `--validation_prompt`s, a\"\n",
    "        \" a single `--validation_prompt` to be used with all `--validation_image`s, or a single\"\n",
    "        \" `--validation_image` that will be used with all `--validation_prompt`s.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_validation_images\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Number of images to be generated for each `--validation_image`, `--validation_prompt` pair\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--validation_steps\",\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help=(\n",
    "        \"Run validation every X steps. Validation consists of running the prompt\"\n",
    "        \" `args.validation_prompt` multiple times: `args.num_validation_images`\"\n",
    "        \" and logging the images.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tracker_project_name\",\n",
    "    type=str,\n",
    "    default=\"train_controlnet\",\n",
    "    help=(\n",
    "        \"The `project_name` argument passed to Accelerator.init_trackers for\"\n",
    "        \" more information see https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ab9c3a-7f80-4bc8-89e4-46bd41114072",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1251194685.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "input_args = [\"--pretrained_model_name_or_path\",\"models/stable-diffusion-v1-5\",\n",
    "              \"--train_data_dir\", \"\",\n",
    "              \"--image_column\",\"\",\n",
    "              \"--conditioning_image_column\",\"\",\n",
    "              \"--caption\",\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d4f37e6-5ff6-4c10-9f0c-47ca408edc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a4897-171c-4c5c-a19d-2548544d54fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0b66b-30ac-4469-8204-36e21d33c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with your training data.\n",
    "# Adjust the keys/fields based on the information needed for control nets training.\n",
    "data = {\n",
    "    \"image_column\": \n",
    "    \"conditioning_image_column\": [\"A sunset over mountains\", \"A calm lake at dawn\"],\n",
    "    \"caption\": [\"edge_detection\", \"depth_map\"]\n",
    "}\n",
    "\n",
    "# Convert dictionaries to Dataset objects.\n",
    "train_dataset = Dataset.from_dict(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6684a0-2c79-41bc-9b78-6a3927decf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_args is not None:\n",
    "    args = parser.parse_args(input_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10b6d64-0d3f-4f4a-85fe-dc2a60e81240",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 314\u001b[0m\n\u001b[1;32m    294\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--validation_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     ),\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--tracker_project_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m     ),\n\u001b[1;32m    312\u001b[0m )\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43minput_args\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(input_args)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_args' is not defined"
     ]
    }
   ],
   "source": [
    "if args.dataset_name is None and args.train_data_dir is None:\n",
    "    raise ValueError(\"Specify either `--dataset_name` or `--train_data_dir`\")\n",
    "\n",
    "if args.proportion_empty_prompts < 0 or args.proportion_empty_prompts > 1:\n",
    "    raise ValueError(\"`--proportion_empty_prompts` must be in the range [0, 1].\")\n",
    "\n",
    "if args.validation_prompt is not None and args.validation_image is None:\n",
    "    raise ValueError(\"`--validation_image` must be set if `--validation_prompt` is set\")\n",
    "\n",
    "if args.validation_prompt is None and args.validation_image is not None:\n",
    "    raise ValueError(\"`--validation_prompt` must be set if `--validation_image` is set\")\n",
    "\n",
    "if (\n",
    "    args.validation_image is not None\n",
    "    and args.validation_prompt is not None\n",
    "    and len(args.validation_image) != 1\n",
    "    and len(args.validation_prompt) != 1\n",
    "    and len(args.validation_image) != len(args.validation_prompt)\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Must provide either 1 `--validation_image`, 1 `--validation_prompt`,\"\n",
    "        \" or the same number of `--validation_prompt`s and `--validation_image`s\"\n",
    "    )\n",
    "\n",
    "if args.resolution % 8 != 0:\n",
    "    raise ValueError(\n",
    "        \"`--resolution` must be divisible by 8 for consistently sized encoded images between the VAE and the controlnet encoder.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae61351-d032-4fdc-b0d6-2ea809943e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb68b9-9a8d-4377-9d15-699d451c439a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cccbab4-d480-4f0e-acbb-b36af378a826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "control"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
